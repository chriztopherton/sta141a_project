---
title: "Edit"
author: "Grant Smith"
date: "12/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Question 1:**
*What is the best combination of variables in predicting diabetes in Pima Indians?*

An intial method we utilized was SVM for determining which variables are best suited to determine Diabetes. The full svm model was calculted and then tuned to increase performance.

The initial model gave the following results:

```{r, echo=FALSE}

#svm full model
#>>>>>>> a069b6d0479eefafb5ea061973707b23cfb9c562

svm.md<-svm(Class~.,data=pm.tr.sc,type= 'C-classification', kernel='linear')
#summary(svm.md)

svm.fullpr<-predict(svm.md,pm.ts.sc)
svm.fm<-table(svm.fullpr,pm.ts.sc$Class)

svmfm.res<-cbind(sum(diag(svm.fm))/sum(svm.fm),#accuracy
svm.fm[4]/sum(svm.fm[,2]), #sensitivity
svm.fm[1]/sum(svm.fm[,1])) #specificity
colnames(svmfm.res)<-c('Accuracy','Sensitivity','Specificity')
list(svm.fm,svmfm.res)
```

Tuning the model in order to further optomize the results yieled a model with improved error and dispersion. Under this model the below results captured the accuracy, sensitivity and specificity when predicting diabetes using SVM with all features. Upon reviewing these results we then proceeded to use feature selection through SVM to improve accuracy. Svm would consistently include glu,bmi,ped, and npreg as the best features to use for maximizing results. On occasion an additional predictor would be included in the model but the lack of consistency lead to our decision to only focus on the features that occured in every iteration of the experiment. 

Even with the best features used in the svm model, we saw little performance increase. The next step we took was to attempt to use different kernal's to see if results improved. We initially used a linear kernal for both the full and feature selection model. Results saw little to no improvement. However, once we applied the kernal change to radial for the reduced model using only the selected features, we typically saw accuracy and specificty both improve. The below code represents one iteration of the process we used. 

```{r,echo=FALSE}
#svm full model 
lin.tn<-tune.svm(Class~.,data=pm.tr.sc,kernel='linear',cost=c(0.001,0.01,0.1,1,5,10))
summary(lin.tn)
```

```{r,echo=FALSE}
#pick model
svm.ln.best<-lin.tn$best.model

tn.ts<-predict(svm.ln.best,newdata=pm.ts.sc)
svm.tn.cm<-table(tn.ts,pm.ts.sc$Class)

ln.res<-cbind(sum(diag(svm.tn.cm))/sum(svm.tn.cm),#accuracy
svm.tn.cm[4]/sum(svm.tn.cm[,2]), #sensitivity
svm.tn.cm[1]/sum(svm.tn.cm[,1])) #specificity
colnames(ln.res)<-c('Accuracy','Sensitivity','Specificity')
list(svm.tn.cm,ln.res)
```

**select features**
```{r,echo=FALSE,,fig.height=3,fig.width=5}

feat<-rfeControl(functions=lrFuncs, method="cv", number=10)
svm.feat <- rfe(pm.tr.sc[,1:7], pm.tr.sc[,8],sizes = c(7, 6, 5, 4),
              rfeControl = feat, method = "svmLinear")

svm.feat
plot(svm.feat)
```

**SVM with selected features**

The following results use glu, bmi, ped, and npreg as predictors. 
```{r,echo=FALSE}

svm.md2<-svm(Class~glu+bmi+ped+npreg,pm.tr.sc,type="C-classification",kernel='linear')
svm.pr2<-predict(svm.md2,pm.ts.sc[,c(1,2,5,6)])

(svm.cm2<-table(svm.pr2,pm.ts.sc[,8]))
res.sv2<-cbind(sum(diag(svm.cm2))/sum(svm.cm2),
svm.cm2[4]/sum(svm.cm2[,2]), #sensitivity
svm.cm2[1]/sum(svm.cm2[,1])) #specificity

colnames(res.sv2)<-c('Accuracy','Sensitivity','Specificity');res.sv2
```

**Change Kernal**

Using the full model with a radial kernel, we only saw specificity increase slightly but accuracy and sensitivity did not improve.

```{r,echo=FALSE}

svm.md4<-svm(Class~.,pm.tr.sc,type="C-classification",kernel='radial')
svm.pr4<-predict(svm.md4,pm.ts.sc[,-8])

svm.cm4<-table(svm.pr4,pm.ts.sc[,8])
res.sv4<-cbind(sum(diag(svm.cm4))/sum(svm.cm4),
svm.cm4[4]/sum(svm.cm4[,2]), #sensitivity
svm.cm4[1]/sum(svm.cm4[,1])) #specificity

colnames(res.sv4)<-c('Accuracy','Sensitivity','Specificity');res.sv4
```

Using the reduced model with a radial kernel yieled the following results. 
```{r,echo=FALSE}
svm.md3<-svm(Class~glu+bmi+ped+npreg,pm.tr.sc,type="C-classification",kernel='radial')
svm.pr3<-predict(svm.md3,pm.ts.sc[,c(1,2,5,6)])

svm.cm3<-table(svm.pr3,pm.ts.sc[,8])
res.sv3<-cbind(sum(diag(svm.cm3))/sum(svm.cm3),
svm.cm3[4]/sum(svm.cm3[,2]), #sensitivity
svm.cm3[1]/sum(svm.cm3[,1])) #specificity

colnames(res.sv3)<-c('Accuracy','Sensitivity','Specificity');res.sv3
```

**Summary Results**
```{r,echo=FALSE}
svm.mats<-list(svm.fm,svm.cm2,svm.cm4,svm.cm3)
svm.results<-list(svmfm.res,res.sv2,res.sv4,res.sv3)
theSVM<-c('Full Linear','Reduced Linear','Full Radial','Reduced Radial')
theSVM2<-c('Full Linear','Reduced Linear','Full Radial','Reduced Radial')
names(svm.results)<-theSVM;names(svm.mats)<-theSVM2

svm.mats;svm.results
```


## KNN

**Full KNN**

Using Knn with all variables yields the following results which perform similarly to SVM.
```{r,echo=FALSE}
library(class)

pm.knn12<-knn(pm.tr.sc[,-8],pm.ts.sc[,-8],pm.tr.sc[,8],k=k.cv)
(pm.kn.cm12<-table(pm.knn12,pm.ts.sc$Class))
list(cbind(sum(diag(pm.kn.cm12)/sum(pm.kn.cm12)),
pm.kn.cm12[4]/sum(pm.kn.cm12[,2]), #sensitivity
pm.kn.cm12[1]/sum(pm.kn.cm12[,1]))) #specificity
```

**KNN:Selected Features**

Using knn with the selected variables from SVM performed higher than using KNN with all features. This further supports SVM's conclusion that using selected features will yield at least as good of a prediction. In regards to selecting K for this method, we used an accuracy optimizer and then cross validation to determine which value of K would yield the best results. These tools are featured in the appendix.

note: While the optimizer would yield differnt results due to randomness in tie breaking, it still gave us a ballpark k that could then be used with cross-validation. This allowed us to save computing power by not testing an excessive number of K-values.
```{r,echo=FALSE}
# K accuracy optimizer
cv.error<-numeric(0)
t<-1
for (i in 1:200){
  
pm.knnK<-knn(pm.tr.sc[,c(1,2,5,6)],pm.ts.sc[,c(1,2,5,6)],pm.tr.sc[,8],k=t)
pm.kn.cmK<-table(pm.knnK,pm.ts.sc$Class)
cv.error[i]<-(sum(diag(pm.kn.cmK))/sum(pm.kn.cmK))

t<-t+1
}
cv.error<-matrix(cv.error)
cv.error.max<-max(cv.error)
k.optm<-which.max(cv.error)
```

```{r,echo=FALSE}
#K optimizer CV method
model1<-train(Class~glu+ped+npreg+bmi,data=pm.tr.sc,method='knn',
              tuneGrid=expand.grid(.k=1:10),
              metric='Accuracy',
              trControl=trainControl(
                method = 'repeatedcv',
                number = 10,
                repeats = 15))
k.cv<-model1$bestTune
```

```{r,echo=FALSE}

pm.knn12<-knn(pm.tr.sc[,c(1,2,5,6)],pm.ts.sc[,c(1,2,5,6)],pm.tr.sc[,8],k=k.cv)
(pm.kn.cm12<-table(pm.knn12,pm.ts.sc$Class))
list(cbind(sum(diag(pm.kn.cm12)/sum(pm.kn.cm12)),
pm.kn.cm12[4]/sum(pm.kn.cm12[,2]), #sensitivity
pm.kn.cm12[1]/sum(pm.kn.cm12[,1]))) #specificity
```
