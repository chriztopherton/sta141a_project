---
title: "Sta141a Final Report"
output: html_document

date: "12/9/2019"

---

```{r setup, include=FALSE,echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(plotly)
library(dplyr)
library(ggbiplot)
library(MASS)
library(ISLR)
library(boot)
library(class)
library(caret)
```


## Collaborators:
  Grant Smith - ggsmith@ucdavis.edu
  Christopher Ton - chrton@ucdavis.edu


## Overview of the problem
We aim to predict whether or not a Pima Indian Woman is diabetic or not based on her health characteristics such as pregnancy, glucose, blood pressure, skin, bmi, pedigree, and age. Additionally, we will elaborate on which biological features (i.e. glucose levels)  are best suited in correctly identifying diabetes in the subject group.  Throughout this analysis we will look at accuracy, sensitivity, and specificity. While we want all of these values to be high, some analysis will emphasize sensitivity over specificity. Due to the medical nature of the data we believe importance should be placed on correctly identifying an individual with the disease as correct and accurate diagnosis plays an important role in proper treatment of patients.

## Exploratory Data Analysis
We first calculated summary statistics for the dataset in order to obtain a clearer picture of the variables we are working with. 

**(Data)**
```{r}
#install.packages('car')
library(MASS)
library(car)

data("Pima.te")

#training and test data (70:30) split
Pim.N<-as.data.frame(rbind(Pima.te,Pima.tr))
trng = sample(1:532, 362, replace=FALSE)
test.g<- setdiff(1:532,trng)

Pim.tr<-Pim.N[trng,]
Pim.te<-Pim.N[test.g,]
```

**(Summary Statistics)**
```{r,echo=FALSE}
summary(Pim.N)
```

# PCA
```{r,echo=FALSE}
pima.pca <- prcomp(Pim.tr[,-8], center = TRUE,scale = TRUE)
summary(pima.pca) # PCI explains 34%,PC2 explains 20%

#cor(pima[,-8])

```

We considered reducing the dimensions of our data by finding candidate variables contributing in direction of the most variance, thus ignoring any of the unnecessary noise in our data. Particularly, we wished to maximize the amount of information along these directions, or namely, the eigenvectors of our dimensions. PC1,PC2,PC3,PC4,PC5 collectively describe at least 90% of the variance. However, due to the lack of high correlation among the variables, PCA fails to determine the minimal number of components to account for the variability and thus, most, if not all, of the predictors may be necessary for the determination of diabetes.


An effective approach with PCA would then pursue LDA, or perhaps QDA based on the determined principal components. To meet the assumptions of discriminant analysis, we tested to see if normality holds. 

# Testing for Normality
```{r,echo=FALSE}

pos.db<-subset(Pima.tr, type=='Yes')
par(mfrow=c(2,4))
for (i in c('glu','age','ped','bmi','npreg','skin','bp'))
{
  qqnorm(pos.db[[i]]);qqline(pos.db[[i]],col=2)
}
```


```{r,echo=FALSE}
pp.st<-numeric(0)
for (i in c('glu','age','ped','bmi','npreg','bp','skin'))
{
s.test<-shapiro.test(pos.db[[i]])
pp.st[i]<-s.test$p.value
}
round(pp.st,4)
```

The shapiro wilks test confirms that only bp, bmi, and glu are normally distributed. (using 0.01 alpha)
```{r,echo=FALSE}
neg.db<-subset(Pima.tr, type=='No')

par(mfrow=c(3,3))
for (i in c('glu','age','ped','bmi','npreg','bp','skin'))
{
qqnorm(neg.db[[i]]);qqline(neg.db[[i]],col=2)
}


#shapiro wilkes test for normality
np.st<-numeric(0)
for (i in c('glu','age','ped','bmi','npreg','bp','skin'))
{
s.test<-shapiro.test(neg.db[[i]])
np.st[i]<-s.test$p.value
}
round(np.st,3)
```
For the group negative for diabetes, we can see from the shapiro wilks test that skin, glu, age, ped, and npreg all violate the normality assumption. Only bp and bmi are normal. (using .01 alpha)

## Conclusion from normality tests

The only two variables that exhibit normality based on the shapiro wilks test for both yes and no diabetes groups are bp and bmi. Therefore we intend to focus on methods that do not rely on the assumption of normality. 

## KNN

KNN only requires that our data be from a random sample. Additionally we will standardize our dataset before carrying out our analysis. The data has been split with 70% of the data being alloted for training and the remaining 30% being used for testing. It is difficult to gain any inference from KNN so we will mostly be using it to confirm other methods conclusions regarding which variables are best for classifying diabetes. 

```{r,echo=FALSE}
#Standardize the varaibles being used
pm.tr.sc<-data.frame(scale(Pim.tr[-8],center = TRUE,scale=TRUE))
pm.tr.sc<-data.frame(cbind(pm.tr.sc),Class=Pim.tr[,8])

pm.ts.sc<-data.frame(scale(Pim.te[-8],center = TRUE,scale=TRUE))
pm.ts.sc<-data.frame(cbind(pm.ts.sc),Class=Pim.te[,8])
```


## Pairwise KNN function

```{r,echo=FALSE}

#used to get all pw results. With k optimizer used to find k that would give highest accuracy

pw.knn<-function(x,y,z) 
{
pm.knn<-knn(pm.tr.sc[,c(x,y)],pm.ts.sc[,c(x,y)],pm.tr.sc[,8],k=z)

pm.kn.cm<-table(pm.knn,pm.ts.sc$Class)
ac<-sum(diag(pm.kn.cm)/sum(pm.kn.cm))
se<-pm.kn.cm[4]/sum(pm.kn.cm[,2])
sp<-pm.kn.cm[1]/sum(pm.kn.cm[,1])
pw.knn.res<-cbind(ac,se,sp)
colnames(pw.knn.res)<-c('accuracy','sensitivity','specificity')
return(list(pm.kn.cm,pw.knn.res))
}

pw.knn(1,2,23)
#test
```

```{r,echo=FALSE}
#K optimizer CV method
# install.packages('caret')
# library(caret)
# 
# names(pm.tr.sc)
# 
# model1<-train(Class~.,data=pm.tr.sc,method='knn',
#               tuneGrid=expand.grid(.k=1:100),
#               metric='Accuracy',
#               trControl=trainControl(
#                 method = 'repeatedcv',
#                 number = 10,
#                 repeats = 15))
# model1$bestTune# 

```


```{r,echo=FALSE}
#K accuracy optimizer

k.opt<-function(x,y)
{
cv.error<-numeric(0)
t<-1
for (i in 1:200){
  
pm.knnK<-knn(pm.tr.sc[,c(x,y)],pm.ts.sc[,c(x,y)],pm.tr.sc[,8],k=t)
pm.kn.cmK<-table(pm.knnK,pm.ts.sc$Class)
cv.error[i]<-(sum(diag(pm.kn.cmK))/sum(pm.kn.cmK))

t<-t+1
}
cv.error<-matrix(cv.error)
cv.error.max<-max(cv.error)
k.optm<-which.max(cv.error)

return(c(cv.error.max,k.optm))
}

k.opt(1,2)

```
**Full KNN**
```{r,echo=FALSE}
library(class)

pm.knn12<-knn(pm.tr.sc[,-8],pm.ts.sc[,-8],pm.tr.sc[,8],k=16)
(pm.kn.cm12<-table(pm.knn12,pm.ts.sc$Class))
sum(diag(pm.kn.cm12)/sum(pm.kn.cm12))
pm.kn.cm12[4]/sum(pm.kn.cm12[,2]) #sensitivity
pm.kn.cm12[1]/sum(pm.kn.cm12[,1]) #specificity
```
Using Knn with all the available information we see that our accuracy, sesnsitive, and specificity are relatively good.

Now we will experiment with dropping number of pregnancies to see if knn still performs as well. 

**KNN drop npreg**
```{r,echo=FALSE}
pm.knn12<-knn(pm.tr.sc[,-c(8,1)],pm.ts.sc[,-c(8,1)],pm.tr.sc[,8],k=46)
(pm.kn.cm12<-table(pm.knn12,pm.ts.sc$Class))
sum(diag(pm.kn.cm12)/sum(pm.kn.cm12))
pm.kn.cm12[4]/sum(pm.kn.cm12[,2]) #sensitivity
pm.kn.cm12[1]/sum(pm.kn.cm12[,1]) #specificity
```
So dropping number of pregnancies decreased accuracy and sensitivity but we say specificity increase. (Pairing npreg with each variable showed different results that indicate pregnancy is not a good indicator for diabetes)

**KNN with logical vars(bp,bmi,skin,glu)**
```{r,echo=FALSE}
pm.knn12<-knn(pm.tr.sc[,-c(8,1,7)],pm.ts.sc[,-c(8,1,7)],pm.tr.sc[,8],k=6)
(pm.kn.cm12<-table(pm.knn12,pm.ts.sc$Class))
sum(diag(pm.kn.cm12)/sum(pm.kn.cm12))
pm.kn.cm12[4]/sum(pm.kn.cm12[,2]) #sensitivity
pm.kn.cm12[1]/sum(pm.kn.cm12[,1]) #specificity
```
Dropping variables npreg and age we still get relatively good results as we did including all variables.

**KNN with logical vars(bp,bmi,skin,age)**
```{r,echo=FALSE}
pm.knn12<-knn(pm.tr.sc[,-c(8,1,7,6)],pm.ts.sc[,-c(8,1,7,6)],pm.tr.sc[,8],k=23)
(pm.kn.cm12<-table(pm.knn12,pm.ts.sc$Class))
sum(diag(pm.kn.cm12)/sum(pm.kn.cm12))
pm.kn.cm12[4]/sum(pm.kn.cm12[,2]) #sensitivity
pm.kn.cm12[1]/sum(pm.kn.cm12[,1]) #specificity
```

**KNN with logical vars(glu,bp,ped,age)**
```{r,echo=FALSE}
names(pm.tr.sc)
pm.knn12<-knn(pm.tr.sc[,-c(8,1,3,4)],pm.ts.sc[,-c(8,1,3,4)],pm.tr.sc[,8],k=102)
(pm.kn.cm12<-table(pm.knn12,pm.ts.sc$Class))
sum(diag(pm.kn.cm12)/sum(pm.kn.cm12))
pm.kn.cm12[4]/sum(pm.kn.cm12[,2]) #sensitivity
pm.kn.cm12[1]/sum(pm.kn.cm12[,1]) #specificity
```
**KNN with logical vars(glu,bp,ped,npreg)**
```{r,echo=FALSE}
names(pm.tr.sc)
pm.knn12<-knn(pm.tr.sc[,c(1,2,5,6)],pm.ts.sc[,c(1,2,5,6)],pm.tr.sc[,8],k=3)
(pm.kn.cm12<-table(pm.knn12,pm.ts.sc$Class))
sum(diag(pm.kn.cm12)/sum(pm.kn.cm12))
pm.kn.cm12[4]/sum(pm.kn.cm12[,2]) #sensitivity
pm.kn.cm12[1]/sum(pm.kn.cm12[,1]) #specificity
```

```{r,echo=FALSE}
names(pm.tr.sc)
pm.knn12<-knn(pm.tr.sc[,c(1,2,4,5,7)],pm.ts.sc[,c(1,2,4,5,7)],pm.tr.sc[,8],k=9)
(pm.kn.cm12<-table(pm.knn12,pm.ts.sc$Class))
sum(diag(pm.kn.cm12)/sum(pm.kn.cm12))
pm.kn.cm12[4]/sum(pm.kn.cm12[,2]) #sensitivity
pm.kn.cm12[1]/sum(pm.kn.cm12[,1]) #specificity
```

**(Glucose and X-var)**
```{r,echo=FALSE}
#Visualize
library(ggplot2)

gg1<-ggplot(mapping=aes(x=glu,y=bmi)) +
	geom_point(data=pm.tr.sc,mapping=aes(colour=Class),size=3) +
	geom_point(data=pm.ts.sc,shape=4,size=3)
```




## Support Vector Machine

Suppoert Vector Machine will be used to help confirm which variables are as important for classification. Confusion table, accuracy, sensitivity, and specificity will all be reported. SVM requires that the data be i.i.d. 


```{r}
#install.packages('e1071', dependencies=TRUE)
#install.packages('kernlab')
library(e1071)
library(kernlab)
library(caret)
```

```{r}
#svm full model
#>>>>>>> a069b6d0479eefafb5ea061973707b23cfb9c562

svm.md<-svm(Class~.,data=pm.tr.sc,type= 'C-classification', kernel='linear')
summary(svm.md)

svm.fullpr<-predict(svm.md,pm.ts.sc)
svm.fm<-table(svm.fullpr,pm.ts.sc$Class)

svmfm.res<-cbind(sum(diag(svm.fm))/sum(svm.fm),#accuracy
svm.fm[4]/sum(svm.fm[,2]), #sensitivity
svm.fm[1]/sum(svm.fm[,1])) #specificity
colnames(svmfm.res)<-c('Accuracy','Sensitivity','Specificity');svmfm.res
```


```{r,echo=FALSE}

#svm full model tune


lin.tn<-tune.svm(Class~.,data=pm.tr.sc,kernel='linear',cost=c(0.001,0.01,0.1,1,5,10))

summary(lin.tn)

svm.md<-svm(Class~.,data=pm.tr.sc,type= 'C-classification', kernel='linear')
summary(svm.md)
```
**pick model**
```{r,echo=FALSE}
svm.ln.best<-lin.tn$best.model
tn.ts<-predict(svm.ln.best,newdata=pm.tr.sc)
(svm.tn.cm<-table(tn.ts,pm.tr.sc$Class))

ln.res<-cbind(sum(diag(svm.tn.cm))/sum(svm.tn.cm),#accuracy
svm.tn.cm[4]/sum(svm.tn.cm[,2]), #sensitivity
svm.tn.cm[1]/sum(svm.tn.cm[,1])) #specificity
colnames(ln.res)<-c('Accuracy','Sensitivity','Specificity');ln.res
```

**select features**
```{r,echo=FALSE}
set.seed(2)

feat<-rfeControl(functions=lrFuncs, method="cv", number=10)

svm.feat <- rfe(pm.tr.sc[,1:7], pm.tr.sc[,8],sizes = c(8, 7, 6, 5, 4),
              rfeControl = feat, method = "svmLinear")

svm.feat
```

**SVM with selected features**
```{r,echo=FALSE}
names(pm.tr.sc)
svm.md2<-svm(Class~glu+bmi+ped+npreg,pm.tr.sc,type="C-classification",kernel='linear')
svm.pr2<-predict(svm.md2,pm.ts.sc[,c(1,2,5,6)])

(svm.cm2<-table(svm.pr2,pm.ts.sc[,8]))
res.sv2<-cbind(sum(diag(svm.cm2))/sum(svm.cm2),
svm.cm2[4]/sum(svm.cm2[,2]), #sensitivity
svm.cm2[1]/sum(svm.cm2[,1])) #specificity

colnames(res.sv2)<-c('Accuracy','Sensitivity','Specificity');res.sv2
```

Svm concludes that the best features to use are glu,bmi,ped, and npreg.

**Change Kernal**
-did not improve full model
```{r,echo=FALSE}
svm.md4<-svm(Class~.,pm.tr.sc,type="C-classification",kernel='radial')
svm.pr4<-predict(svm.md4,pm.ts.sc[,-8])

(svm.cm4<-table(svm.pr4,pm.ts.sc[,8]))
res.sv4<-cbind(sum(diag(svm.cm4))/sum(svm.cm4),
svm.cm4[4]/sum(svm.cm4[,2]), #sensitivity
svm.cm4[1]/sum(svm.cm4[,1])) #specificity

colnames(res.sv4)<-c('Accuracy','Sensitivity','Specificity');res.sv4
```



-radial kernal improved the reduced svm model
```{r,echo=FALSE}
svm.md3<-svm(Class~glu+bmi+ped+npreg,pm.tr.sc,type="C-classification",kernel='radial')
svm.pr3<-predict(svm.md3,pm.ts.sc[,c(1,2,5,6)])

(svm.cm3<-table(svm.pr3,pm.ts.sc[,8]))
res.sv3<-cbind(sum(diag(svm.cm3))/sum(svm.cm3),
svm.cm3[4]/sum(svm.cm3[,2]), #sensitivity
svm.cm3[1]/sum(svm.cm3[,1])) #specificity

colnames(res.sv3)<-c('Accuracy','Sensitivity','Specificity');res.sv3
```

```{r,echo=FALSE}
svm.mats<-list(svm.fm,svm.cm2,svm.cm4,svm.cm3)
svm.results<-list(res.sv1,res.sv2,res.sv4,res.sv3)
theSVM<-c('Full Linear','Reduced Linear','Full Radial','Reduced Radial')
theSVM2<-c('Full Linear','Reduced Linear','Full Radial','Reduced Radial')
names(svm.results)<-theSVM
names(svm.mats)<-theSVM2
# svm.results$`Full Linear`<-list(res.sv1,svm.cm)
# svm.results$`Reduced Linear`<-list(res.sv2,svm.cm2)
# svm.results$`Full Radial`<-list(res.sv4,svm.cm4)
# svm.results$`Reduced Radial`<-list(res.sv3,svm.cm3)

svm.mats
svm.results
```

## Random Forest

```{r}
set.seed(1234)
s.ptr <- data.frame(cbind(scale(Pim.tr[-8],center=TRUE)),Pim.tr[8])
s.pte <- data.frame(cbind(scale(Pim.te[-8],center=TRUE)),Pim.te[8])

(rf <- randomForest(type ~. ,data=s.ptr,ntree=1000,importance=TRUE))
pred <- predict(rf,newdata=s.pte[-8])

#cm
(rf.cm <-table(s.pte[,8],pred))
res.rf <-cbind(sum(diag(rf.cm))/sum(rf.cm),
rf.cm[4]/sum(rf.cm[,2]), #sensitivity
rf.cm[1]/sum(rf.cm[,1])) #specificity

colnames(res.rf)<-c('Accuracy','Sensitivity','Specificity');res.rf

#variable importance
(imp_var <- data.frame(randomForest::importance(rf)))
varImpPlot(rf)

```

** Illustration of Decision Tree **
```{r,fig.height=4}
require(rpart)
fit <- rpart(pima.train$type ~.,pima.train[,-8])
plot(fit)
text(fit)
#test
```

### R Appendix
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```